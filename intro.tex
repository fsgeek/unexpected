%%
\IEEEPARstart{N}{on-volatile} memory research began decades ago~\cite{wu1994envy}.
The April 2019 release of Intel Optane\textsuperscript{\tiny TM} DC Persistent Memory (PMEM) has accelerated
that research. Prior to availability, most work was done via emulation~\cite{wu1994envy,Maciejewski2017persistent,dulloor2014system,volos2015quartz,doudali2017comerge}.  We expected PMEM to exhibit lower bandwidth and higher latency compared to DRAM; work with PMEM is consistent with these expectations~\cite{gill2019single,izraelevitz2019basic}.  Yet we also note the actual performance behavior is more complex~\cite{peng2019system}. 
%%

%%
We evaluated PMEM using \textit{Polybench}, a suite of well-known microbenchmarks designed to measure the efficiency of memory locality optimization techniques.  Our initial results, shown in Figure \ref{fig:polybench}, defied the findings of our prior emulation study~\cite{doudali2017comerge}.  We investigated this behavior to better understand the performance profile of PMEM. We confirmed that locality is critical and identified multiple ways in which locality manifests itself in current PMEM systems.  We verified our observations on multiple different systems.  We report our results on the last system that we evaluated.
%%

%%
We observe that the most significant effect of locality is related to default memory management policy, with secondary effects from the PMEM memory controller's use of striping, read-ahead, and write-behind caching.  The Linux default memory management policy is to use the largest possible pages for PMEM, while it defaults to 4KB pages for DRAM.  DAX-aware file systems (for application sharing, dynamic allocation, and security) interact with this policy as well.  We observed that the impact of this is up to 5x and we conclude that those publishing  PMEM evaluations should control for page size. PMEM striping primarily benefits workloads with good data locality (within the stripe size) and high bandwidth requirements.  PMEM without striping often performs similarly when high bandwidth is less critical.  PMEM caching in the memory controller and memory modules benefits CPUs with smaller caches.  Once we controlled for page size and used a memory locality optimizing tool, we confirmed performance for PMEM was as good or better than expectations.  We achieved this performance from careful optimization, \textit{not} by using default configurations.
%%

\begin{figure}[!htb]
    \captionsetup{justification=centering}
    \centering
    \caption{Polybench: Performance of Striped PMEM relative to DRAM, Linux 5.0 kernel.  Surprising results showing better performance on PMEM are highlighted in \textbf{bold}. Sorted by PMEM performance.}
    \vspace{0.1cm}
    \label{fig:polybench}
    \begin{tabular}{c}
        \includegraphics[width=0.495\textwidth]{pbfig-dec2019.eps}
    \end{tabular}
\end{figure}

