%!TEX program = lualatex
\documentclass[10pt,journal,compsoc]{IEEEtran}

\usepackage{ifluatex}
\ifCLASSOPTIONcompsoc
  \usepackage[nocompress]{cite}
\else
  \usepackage{cite}
\fi

\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
\else
  \usepackage{graphicx}
\fi

\usepackage{array}

\usepackage{siunitx}
\usepackage{adjustbox}


\ifCLASSOPTIONcompsoc
  \usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
\else
  \usepackage[caption=false,font=footnotesize]{subfig}
\fi


\usepackage{stfloats}

\usepackage{url}

\usepackage{comment}
\usepackage{balance}

%\newcommand{\ada}[1]{\textcolor{red}{#1}}
\newcommand{\ada}[1]{}

\usepackage{tikz}
\usepackage[utf8]{inputenc}
\ifluatex
\usepackage{fontspec} % This line only for XeLaTeX and LuaLaTeX
\fi
\usepackage{pgfplots}
\pgfplotsset{compat=1.16}
\usepackage{color, colortbl}
\usetikzlibrary{svg.path}
\usepackage{scalerel}

\definecolor{orcidlogocol}{HTML}{A6CE39}
\tikzset{
  orcidlogo/.pic={
    \fill[orcidlogocol] svg{M256,128c0,70.7-57.3,128-128,128C57.3,256,0,198.7,0,128C0,57.3,57.3,0,128,0C198.7,0,256,57.3,256,128z};
    \fill[white] svg{M86.3,186.2H70.9V79.1h15.4v48.4V186.2z}
                 svg{M108.9,79.1h41.6c39.6,0,57,28.3,57,53.6c0,27.5-21.5,53.6-56.8,53.6h-41.8V79.1z M124.3,172.4h24.5c34.9,0,42.9-26.5,42.9-39.7c0-21.5-13.7-39.7-43.7-39.7h-23.7V172.4z}
                 svg{M88.7,56.8c0,5.5-4.5,10.1-10.1,10.1c-5.6,0-10.1-4.6-10.1-10.1c0-5.6,4.5-10.1,10.1-10.1C84.2,46.7,88.7,51.3,88.7,56.8z};
  }
}

\newcommand\orcidicon[1]{\href{https://orcid.org/#1}{\mbox{\scalerel*{
\begin{tikzpicture}[yscale=-1,transform shape]
\pic{orcidlogo};
\end{tikzpicture}
}{|}}}}

\hyphenation{op-tical net-works semi-conduc-tor}

\usepackage[bookmarks=false]{hyperref} %<-- load after all other packages

\begin{document}

\title{Unexpected Performance of Intel\textsuperscript{\textregistered} Optane\textsuperscript{\small TM}
DC Persistent Memory}

\author{Tony~Mason~\orcidicon{0000-0002-0651-5019},~\IEEEmembership{Student~Member,~IEEE,}
Thaleia~Dimitra~Doudali~\orcidicon{0000-0002-3197-839X},~\IEEEmembership{Student~Member,~IEEE}, \\
Margo~Seltzer~\orcidicon{0000-0002-2165-4658},
        and~Ada~Gavrilovska~\orcidicon{0000-0003-4199-2512},~\IEEEmembership{Member,~IEEE}% <-this % stops a space
\thanks{T. Mason and M. Seltzer are with the Department of Computer Science, University of British Columbia, Vancouver, BC Canada e-mail: fsgeek@cs.ubc.ca, mseltzer@cs.ubc.ca.}% <-this % stops a space
\thanks{T. Mason, T. Doudali, and A. Gavrilovska are with the College of Computing, Georgia Institute of Technology, Atlanta,
GA, 30332 USA e-mail: fsgeek@gatech.edu, thdoudali@gatech.edu, ada@cc.gatech.edu.}% <-this % stops a space
\thanks{Manuscript received August 13, 2019; revised \today.}
}

\markboth{IEEE COMPUTER ARCHITECTURE LETTERS}%
{Mason \MakeLowercase{\textit{et al.} Unexpected Performance of Intel\textsuperscript{\textregistered} Optane\textsuperscript{\tiny TM}
DC Persistent Memory}}

\IEEEtitleabstractindextext{%
\begin{abstract}
  %%
  We evaluated Intel\textsuperscript{\textregistered} Optane\textsuperscript{\tiny TM} DC Persistent Memory and found that Intel's persistent memory is highly sensitive to data locality, size, and access patterns, which becomes clearer by optimizing both virtual memory page size and data layout for locality. Using the \textit{Polybench} high-performance computing benchmark suite and controlling for mapped page size, we evaluate persistent meemory (PMEM) performance relative to DRAM. In particular, the Linux PMEM support maps preferentially maps persistent memory in large pages while always mapping DRAM to small pages.  We observed using large pages for PMEM and small pages for DRAM can create a 5x difference in performance, dwarfing other effects discussed in the literature.  We found PMEM performance comparable to DRAM performance for the majority of tests when controlled for page size and optimized for data locality.
  %%  
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Persistent Memory
\end{IEEEkeywords}
}%


\maketitle


\IEEEdisplaynontitleabstractindextext
\IEEEpeerreviewmaketitle

\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}

\balance

\input{intro}

\section{Background}\label{sec:methodology}
\input{methodology}

\section{Evaluation}\label{sec:evaluation}
\input{evaluation}


\section{Conclusion}

%%
We found PMEM is more sensitive to memory locality than DRAM; this is due to differences in the hardware implementing these memory technologies.  The Linux policy to use large pages by default with PMEM provides better TLB and page table locality, which translates to better performance.  Given that large dataset usage is an important use for PMEM, it makes sense to optimize for efficient TLB and last level cache usage rather than demand paging.  Interleaved PMEM (iPMEM) yields higher bandwith than non-interleaved PMEM, but non-interleaved PMEM benefits more than iPMEM from the polyehedral optimizers data locality improvements.
%%

%%
We found it was critical for proper evaluation to ensure properly controlling for Linux page size handling to ensure our comparisons between DRAM and PMEM were valid. We observed that locality optimization, both for virtual address translation and memory access, were critical to achieving optimal results with PMEM.  We also observed that the benefits of various optimizations differed substantially across workloads when using PMEM. We found substantial performance penalties for \textit{not} optimizing with PMEM.
%%

%%
While our evaluation was done with the first generation of commercially available PMEM, we expect these insights will generalize to future implementations of PMEM, particularly those related to the impact of data locality, OS behavior, PMEM caching, and PMEM interleaving.
%%


\ifCLASSOPTIONcompsoc
  \section*{Acknowledgments}
\else
  \section*{Acknowledgment}
\fi

%%
We thank the anonymous reviewers for their valuable feedback, as well as Vaastav Anand, Swati Goswami, Nodir Kodirov, Joel Nider, Ranjan Sarpangala Venkatesh, and Brian Veraa for their assistance in reviewing drafts. This research was partially funded under NSF award 1822972, US Dept. of Energy UNITY, Intel Corporation, and Exascale Computing Project SICM (Simplified Interface to Complex Memory).
%%

\nocite{Hunter:2007} % this is the matplotlib reference
\nocite{yang2019empirical} % latest UCSD paper that includes EWR
\nocite{peng2019system} % LLNL MEMSYS paper
\nocite{yuki2014understanding} % Understanding Polybench
\nocite{yuki2015polybench} % Polybench 4.0
\nocite{xu2016nova} % NOVA File System, damn it all to hell

\bibliographystyle{IEEEtranS}
\bibliography{calpub}
\end{document}
