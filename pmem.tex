%!TEX program = lualatex
\documentclass[10pt,journal,compsoc]{IEEEtran}

\usepackage{ifluatex}
\ifCLASSOPTIONcompsoc
  \usepackage[nocompress]{cite}
\else
  \usepackage{cite}
\fi

\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
\else
  \usepackage{graphicx}
\fi

\usepackage{array}

\usepackage{siunitx}
\usepackage{adjustbox}


\ifCLASSOPTIONcompsoc
  \usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
\else
  \usepackage[caption=false,font=footnotesize]{subfig}
\fi


\usepackage{stfloats}

\usepackage{url}

\usepackage{comment}
\usepackage{balance}

%\newcommand{\ada}[1]{\textcolor{red}{#1}}
\newcommand{\ada}[1]{}

\usepackage{tikz}
\usepackage[utf8]{inputenc}
\ifluatex
\usepackage{fontspec} % This line only for XeLaTeX and LuaLaTeX
\fi
\usepackage{pgfplots}
\pgfplotsset{compat=1.16}
\usepackage{color, colortbl}
\usetikzlibrary{svg.path}
\usepackage{scalerel}

\definecolor{orcidlogocol}{HTML}{A6CE39}
\tikzset{
  orcidlogo/.pic={
    \fill[orcidlogocol] svg{M256,128c0,70.7-57.3,128-128,128C57.3,256,0,198.7,0,128C0,57.3,57.3,0,128,0C198.7,0,256,57.3,256,128z};
    \fill[white] svg{M86.3,186.2H70.9V79.1h15.4v48.4V186.2z}
                 svg{M108.9,79.1h41.6c39.6,0,57,28.3,57,53.6c0,27.5-21.5,53.6-56.8,53.6h-41.8V79.1z M124.3,172.4h24.5c34.9,0,42.9-26.5,42.9-39.7c0-21.5-13.7-39.7-43.7-39.7h-23.7V172.4z}
                 svg{M88.7,56.8c0,5.5-4.5,10.1-10.1,10.1c-5.6,0-10.1-4.6-10.1-10.1c0-5.6,4.5-10.1,10.1-10.1C84.2,46.7,88.7,51.3,88.7,56.8z};
  }
}

\newcommand\orcidicon[1]{\href{https://orcid.org/#1}{\mbox{\scalerel*{
\begin{tikzpicture}[yscale=-1,transform shape]
\pic{orcidlogo};
\end{tikzpicture}
}{|}}}}

\hyphenation{op-tical net-works semi-conduc-tor}

\usepackage[bookmarks=false]{hyperref} %<-- load after all other packages

\begin{document}

\title{Unexpected Performance of Intel\textsuperscript{\textregistered} Optane\textsuperscript{\small TM}
DC Persistent Memory}

\author{Tony~Mason~\orcidicon{0000-0002-0651-5019},~\IEEEmembership{Student~Member,~IEEE,}
Thaleia~Dimitra~Doudali~\orcidicon{0000-0002-3197-839X},~\IEEEmembership{Student~Member,~IEEE}, \\
Margo~Seltzer~\orcidicon{0000-0002-2165-4658},
        and~Ada~Gavrilovska~\orcidicon{0000-0003-4199-2512},~\IEEEmembership{Member,~IEEE}% <-this % stops a space
\thanks{T. Mason and M. Seltzer are with the Department of Computer Science, University of British Columbia, Vancouver, BC Canada e-mail: fsgeek@cs.ubc.ca, mseltzer@cs.ubc.ca.}% <-this % stops a space
\thanks{T. Mason, T. Doudali, and A. Gavrilovska are with the College of Computing, Georgia Institute of Technology, Atlanta,
GA, 30332 USA e-mail: fsgeek@gatech.edu, thdoudali@gatech.edu, ada@cc.gatech.edu.}% <-this % stops a space
\thanks{Manuscript received August 13, 2019; revised \today.}
}

\markboth{IEEE COMPUTER ARCHITECTURE LETTERS}%
{Mason \MakeLowercase{\textit{et al.} Unexpected Performance of Intel\textsuperscript{\textregistered} Optane\textsuperscript{\tiny TM}
DC Persistent Memory}}

\IEEEtitleabstractindextext{%
\begin{abstract}
  %%
  We evaluated Intel\textsuperscript{\textregistered} Optane\textsuperscript{\tiny TM} DC Persistent Memory and found that Intel's persistent memory is highly sensitive to data locality, size, and access patterns, which becomes clearer by optimizing both virtual memory page size and data layout for locality. Using the \textit{Polybench} high-performance computing benchmark suite and controlling for mapped page size, we evaluate PMEM performance relative to DRAM. In particular, the Linux persistent memory (PMEM) support automatically maps persistent memory in large pages.  The effect of large pages for PMEM and small pages for DRAM is enormous, dwarfing other effects discussed in the literature.  We found PMEM performance comparable to DRAM performance for a number of tests with significant performance improvements when optimized for data locality.
  %%  
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Persistent Memory
\end{IEEEkeywords}
}%


\maketitle


\IEEEdisplaynontitleabstractindextext
\IEEEpeerreviewmaketitle

\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}

\balance

\input{intro}

\section{Background}\label{sec:methodology}
\input{methodology}

\section{Evaluation}\label{sec:evaluation}
\input{evaluation}


\begin{comment}
\section{Future Work}

Our results suggest a number of areas for future work: (1) we need to evaluate the existing polyhedral optimizations to determine why they do not benefit $\approx \frac{1}{3}$ of the Polybench tests.  We expect we will use detailed study of PMEM behavior to better understand the impact on data locality.  (2) We anticipate better understanding of dataset partitioning across multiple caching layers with different optimization functions could yield better insights into developing memory optimization tools. (3) We theorize that memory optimizaiton tools should provide multiple potential implementations optimized to explit specific memory topologies.  (4) We propose developing a DAX-optimized file system that focuses on providing optimal performance of PMEM as well as application sharing, dynamic allocation, and security focused on maximizing performance of applications using PMEM via the DAX model.
\end{comment}

\section{Conclusion}

%%
We found PMEM is more sensitive to memory locality than DRAM; this is due to differences in the hardware implementing these memory technologies.  The decision to default PMEM to large pages provides a substantial boost in page locality.  As datasets grow in size, using large pages yields better TLB locality because more actual PMEM can be described with a fixed number of TLB entries.  PMEM's support for interleaving is most beneficial for data with both good locality and high bandwidth requirements; non-interleaved PMEM works comparably well to interleaved PMEM with good data locality and moderate bandwidth requirements, such as with CPUs employing larger caches.
%%

%%
We caution those evaluating PMEM to ensure the evaluation properly controls for page size because it's impact may lead to incorrect results.  Those using PMEM should consider optimizing data locality as  it can lead to substantial performance gains.  However, those gains are clearly sensitive to the behavior of the underlying memory and thus are dependent upon the workload. Not optimizing risks sacrificing performance when using PMEM.
%%

%%
While our evaluation was done with the first generation of commercially available PMEM, we expect these insights will generalize to future implementations of PMEM, particularly those related to the impact of data locality, OS behavior, and caching/striping (interleaving) of PMEM.
%%


\ifCLASSOPTIONcompsoc
  \section*{Acknowledgments}
\else
  \section*{Acknowledgment}
\fi

%%
We thank the anonymous reviewers for their valuable feedback, as well as Vaastav Anand, Swati Goswami, Nodir Kodirov, Joel Nider, Ranjan Sarpangala Venkatesh, and Brian Veraa for their assistance in reviewing drafts. This research was partially funded under NSF award 1822972, US Dept. of Energy UNITY, Intel Corporation, and Exascale Computing Project SICM (Simplified Interface to Complex Memory).
%%

\nocite{Hunter:2007} % this is the matplotlib reference
\nocite{yang2019empirical} % latest UCSD paper that includes EWR
\nocite{peng2019system} % LLNL MEMSYS paper
\nocite{yuki2014understanding} % Understanding Polybench
\nocite{yuki2015polybench} % Polybench 4.0
\nocite{xu2016nova} % NOVA File System, damn it all to hell

\bibliographystyle{IEEEtranS}
\bibliography{calpub}
\end{document}
